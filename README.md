---
title: Museum Audio Guide
emoji: ğŸ¨
colorFrom: blue
colorTo: purple
sdk: gradio
sdk_version: 6.0.1
app_file: app.py
pinned: false
license: mit
---

# Museum Audio Guide ğŸ¨ğŸ”Š

AI-powered museum guide that makes art accessible through computer vision, conversational AI, and text-to-speech. Built with accessibility in mind for blind and visually impaired visitors.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

ğŸš€ **[Try the Live Demo on Hugging Face](https://huggingface.co/spaces/FeePieper/museum_guide_app)**

---

## ğŸ¯ Overview

Upload a photo of any artwork and receive:
- AI-generated description (visual elements, style, mood)
- Artwork metadata (artist, title, period, historical context)
- Interactive Q&A about techniques, symbolism, and meaning
- Audio narration for hands-free accessibility

**Solution:** Multi-modal AI system combining GPT-4o Vision, RAG vector search, conversational AI, and text-to-speech.

---

## âœ¨ Key Features

### 1. Dual-Mode Interface
- **Audio-Guide Mode**: Automated audio playback, voice-based Q&A
- **Visual-Guide Mode**: Text display, metadata cards, text-based chat

### 2. Multi-Tier Recognition System (35s â†’ 6-8s)
```
Tier 1: Perceptual Hash    (~0.25s)  âš¡ 95% of known artworks
Tier 2: Pre-check          (~0.05s)  âš¡ Filters non-artworks
Tier 3: RAG Vector Search  (2.5s)    ğŸ” Semantic similarity
Tier 4: Vision API         (2-3s)    ğŸ¤– Universal fallback
```

### 3. Optimized Performance
- Chat responses: 5s â†’ 1-2s (60% faster)
- Token usage: 620 â†’ 190 tokens (68% reduction)
- API cost: 71% cheaper per request

---

## ğŸ—ï¸ Architecture

```
Gradio Interface â†’ Core Engine â†’ Cache/Services â†’ OpenAI + Pinecone
                   (Multi-tier   (Hash cache)     (Vision, TTS,
                    recognition,  (Service layer)   Chat, RAG)
                    parallel API,
                    retry logic)
```

### Key Design Decisions

**Multi-Tier Recognition:** Hash â†’ Pre-check â†’ RAG â†’ Vision API (95% cache hit, 80% cost reduction)

**Parallel Processing:** ThreadPoolExecutor for concurrent API calls (2x speedup)

**Token Optimization:** Reduced prompts from 620 â†’ 190 tokens (71% cost savings)

**Tech Stack:** Gradio (rapid prototyping), GPT-4o-mini (speed/cost balance), Pinecone (managed vector DB)

---

## ğŸ› ï¸ Technology Stack

- **Gradio 6.0.1** - Web interface
- **OpenAI GPT-4o-mini** - Vision API, Chat API, TTS API
- **Pinecone 8.0.0** - Vector database (RAG)
- **Pillow 10.1.0** - Image processing
- **imagehash 4.3.1** - Perceptual hashing

**Production features:** Environment config, JSON logging, health checks, retry logic, rate limiting

---

## ğŸš€ Getting Started

### Prerequisites
- Python 3.10+
- [OpenAI API key](https://platform.openai.com/api-keys)
- [Pinecone API key](https://www.pinecone.io/)

### Installation

```bash
# Clone repository
git clone https://github.com/your-username/museum_guide_app.git
cd museum_guide_app

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Add your API keys to .env:
# OPENAI_API_KEY=sk-...
# PINECONE_API_KEY=pcsk-...

# Run application
python app.py
```

Access at **http://localhost:7860**

### Docker

```bash
docker build -t museum-guide:latest .
docker run -d --name museum-guide -p 7860:7860 --env-file .env museum-guide:latest
```

---

## ğŸ’¡ Usage

**Audio-Guide Mode** (blind/visually impaired visitors):
1. Upload artwork photo
2. Automatic audio playback (description + metadata)
3. Voice-based Q&A

**Visual-Guide Mode** (sighted visitors):
1. Upload artwork photo
2. Read description and metadata
3. Text-based chat ("What techniques did the artist use?", "What does this symbolize?")

---

## âš™ï¸ Configuration

Key environment variables in `.env`:

```bash
OPENAI_API_KEY=sk-...
PINECONE_API_KEY=pcsk-...
ENVIRONMENT=development  # or production, testing
LOG_LEVEL=INFO
PORT=7860
```

---

## ğŸ“ Project Structure

```
museum_guide_app/
â”œâ”€â”€ app.py                     # Main Gradio application
â”œâ”€â”€ config.py                  # Environment configuration
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ LICENSE                    # MIT License
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ AI_EVALUATION_REPORT.md    # AI quality test results
â”œâ”€â”€ .env.example               # Environment variables template
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/                  # Core business logic
â”‚   â”‚   â”œâ”€â”€ analyze.py         # Multi-tier artwork recognition
â”‚   â”‚   â”œâ”€â”€ error_handler.py   # Retry logic & error handling
â”‚   â”‚   â”œâ”€â”€ health_check.py    # Health monitoring
â”‚   â”‚   â””â”€â”€ logging_config.py  # Logging configuration
â”‚   â”œâ”€â”€ services/              # External service integrations
â”‚   â”‚   â”œâ”€â”€ vision.py          # OpenAI Vision API
â”‚   â”‚   â”œâ”€â”€ audio.py           # OpenAI TTS API
â”‚   â”‚   â”œâ”€â”€ chat.py            # OpenAI Chat API
â”‚   â”‚   â”œâ”€â”€ rag_database.py    # Pinecone vector database
â”‚   â”‚   â””â”€â”€ image_similarity.py # Perceptual hashing
â”‚   â””â”€â”€ models/                # Data models
â”‚       â””â”€â”€ types.py           # Type definitions
â”œâ”€â”€ tests/                     # Testing & evaluation
â”‚   â”œâ”€â”€ test_ai_quality.py     # AI quality test suite
â”‚   â”œâ”€â”€ test_data/             # Test images
â”‚   â”‚   â”œâ”€â”€ known_artworks/    # Vision API tests
â”‚   â”‚   â”œâ”€â”€ RAG_images/        # RAG database tests
â”‚   â”‚   â””â”€â”€ generic_images/    # Hallucination tests
â”‚   â””â”€â”€ results/               # Test results (JSON)
â”œâ”€â”€ data/                      # Application data
â”‚   â”œâ”€â”€ RAG_database/          # Artwork images for RAG
â”‚   â”œâ”€â”€ image_hash_index.json # Perceptual hash index
â”‚   â””â”€â”€ test_paintings/        # Test artwork images
â”œâ”€â”€ outputs/                   # Generated outputs
â”‚   â””â”€â”€ audio/                 # TTS audio files
â””â”€â”€ logs/                      # Application logs
    â””â”€â”€ app.log                # Main log file
```

---

## ğŸ§ª Testing & Evaluation

### AI Quality Testing

**4-metric testing framework:**

1. **Accuracy**: 100% (6/6 artworks - 3 Vision API + 3 RAG)
2. **Hallucination Detection**: 0% (perfect rejection of non-artworks)
3. **Context Relevancy**: 100% (all chat responses stay on-topic)
4. **Performance**: Vision API ~14s, RAG <1s

**Run tests:**
```bash
python tests/test_ai_quality.py
```

**Full report:** [AI_EVALUATION_REPORT.md](AI_EVALUATION_REPORT.md)

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Credits & Acknowledgments

**Developer:** Fee Pieper

**External APIs & Services:**
- [OpenAI](https://openai.com/) - GPT-4o-mini Vision API, Chat API, TTS API
- [Pinecone](https://www.pinecone.io/) - Vector database infrastructure

**Libraries & Frameworks:**
- [Gradio](https://www.gradio.app/) - Web interface framework
- [Pillow](https://python-pillow.org/) - Image processing library
- [ImageHash](https://github.com/JohannesBuchner/imagehash) - Perceptual hashing

**Documentation & Resources:**
- [OpenAI API Documentation](https://platform.openai.com/docs)
- [Pinecone Documentation](https://docs.pinecone.io)
- [WCAG 2.1 Guidelines](https://www.w3.org/WAI/WCAG21/quickref/) - Accessibility standards

---

**Built for accessible, interactive art education**

*Making museums accessible to everyone, one artwork at a time.*
